{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from plotter_callback import PlotterCallback\n",
    "import deepxde as dde\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from generate_data import get_data\n",
    "from msfnn import MsFNN\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "checkpoint_interval = 10_000\n",
    "\n",
    "# Create the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Add the command line argument\n",
    "parser.add_argument(\n",
    "    \"--checkpoint-interval\", type=int, help=\"Interval for saving plots/checkpoints.\"\n",
    ")\n",
    "\n",
    "# Parse the arguments\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Access the value of the command line argument\n",
    "checkpoint_interval = args.checkpoint_interval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"data\"\n",
    "if not os.path.exists(fn):\n",
    "    get_data()\n",
    "dirs = os.listdir(fn)\n",
    "data = {\n",
    "    name.split(\".\")[0]: np.loadtxt(os.path.join(fn, name), max_rows=290)\n",
    "    for name in dirs\n",
    "}\n",
    "\n",
    "# Use FDM to get accelerations\n",
    "acc_3 = (data[\"Vel_3_2D\"][1:] - data[\"Vel_3_2D\"][:-1]) / (\n",
    "    data[\"t\"][1:] - data[\"t\"][:-1]\n",
    ")\n",
    "acc_4 = (data[\"Vel_4_2D\"][1:] - data[\"Vel_4_2D\"][:-1]) / (\n",
    "    data[\"t\"][1:] - data[\"t\"][:-1]\n",
    ")\n",
    "\n",
    "data[\"Acc_3_2D\"] = np.zeros_like(data[\"t\"])\n",
    "data[\"Acc_4_2D\"] = np.zeros_like(data[\"t\"])\n",
    "\n",
    "data[\"Acc_3_2D\"][1:] = acc_3\n",
    "data[\"Acc_4_2D\"][1:] = acc_4\n",
    "\n",
    "# Normalize Data\n",
    "T_MAX = data[\"t\"].max()\n",
    "U_MAX = max(data[\"Disp_3_2D\"].max(), data[\"Disp_4_2D\"].max())\n",
    "\n",
    "data[\"t\"] /= T_MAX\n",
    "for name in data:\n",
    "    if \"Vel\" in name or \"Disp\" in name:\n",
    "        data[name] /= U_MAX\n",
    "\n",
    "# For convenience\n",
    "a0, a1 = data[\"Damp_param\"]\n",
    "\n",
    "# Get Constant Tensors ready\n",
    "M = torch.Tensor(data[\"M\"])\n",
    "Kb = torch.Tensor(data[\"k_basis\"])\n",
    "\n",
    "\n",
    "# Define interpolation of F. Returns M x N_DIM tensor.\n",
    "def load(t: torch.Tensor):\n",
    "    x = t\n",
    "    xp = data[\"t\"]\n",
    "    fp = data[\"load\"]\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy().squeeze()\n",
    "        f = np.interp(x, xp, fp)\n",
    "        f = torch.Tensor(f)\n",
    "        ret = torch.zeros(t.shape[0], 4)\n",
    "        ret[:, 3] = f\n",
    "        ret = ret.permute((1, 0))\n",
    "    else:\n",
    "        f = np.interp(x, xp, fp).squeeze()\n",
    "        ret = np.zeros((t.shape[0], 4))\n",
    "        ret[:, 3] = f\n",
    "        ret = ret.T\n",
    "    return ret * 1e3  # convert kN -> N"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometry - just an interval\n",
    "geom = dde.geometry.TimeDomain(data[\"t\"].min(), data[\"t\"].max())\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def get_u_derivatives(t: torch.Tensor, u: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "    u_t, u_tt = [u * 0] * 2\n",
    "    for dim in range(int(u.shape[1])):\n",
    "        u_t[:, dim] = dde.grad.jacobian(u, t, i=dim).squeeze()\n",
    "        u_tt[:, dim] = dde.grad.hessian(u, t, component=dim).squeeze()\n",
    "    return u_t, u_tt\n",
    "\n",
    "\n",
    "# Learnable parameter/s\n",
    "E = dde.Variable(0.6)\n",
    "\n",
    "\n",
    "# ODE definition\n",
    "def ode_sys(t, u):\n",
    "    F = load(t)\n",
    "    K = Kb * torch.abs(E)\n",
    "    C = a0 * M + a1 * K\n",
    "\n",
    "    y_t, y_tt = get_u_derivatives(t, u)\n",
    "    y = u.permute((1, 0))\n",
    "    y_t = y_t.permute((1, 0))\n",
    "    y_tt = y_tt.permute((1, 0))\n",
    "\n",
    "    # Whatever E is learned to be, it is actually 1e8 times that value\n",
    "    U = y * U_MAX\n",
    "    DU_DT = y_t * U_MAX / T_MAX\n",
    "    D2U_DT2 = y_tt * U_MAX / T_MAX**2\n",
    "\n",
    "    mass_term = M @ D2U_DT2 / 1e8\n",
    "    damp_term = (a1 * Kb * E) @ DU_DT + (a0 * M) / 1e8 @ DU_DT\n",
    "    stiff_term = (Kb * E) @ U\n",
    "    force_term = F / 1e8\n",
    "    residual = mass_term + damp_term + stiff_term - force_term\n",
    "    residual = residual.permute((1, 0))\n",
    "    return residual\n",
    "\n",
    "\n",
    "# Boundary conditions definition\n",
    "def differentiate_output(t, u, component, order):\n",
    "    if order == 1:\n",
    "        return dde.grad.jacobian(u, t, i=component)\n",
    "    return dde.grad.hessian(u, t, component=component)\n",
    "\n",
    "\n",
    "t_data = data[\"t\"].reshape(-1, 1)\n",
    "zero_vector = np.array([[0]])\n",
    "\n",
    "# IC for unknown dimensions\n",
    "v0 = [\n",
    "    dde.icbc.PointSetOperatorBC(\n",
    "        zero_vector, zero_vector, lambda t, u, X: differentiate_output(t, u, i, 1)\n",
    "    )\n",
    "    for i in (0, 1)\n",
    "]\n",
    "\n",
    "# Known dimensions\n",
    "vi = [\n",
    "    dde.icbc.PointSetOperatorBC(\n",
    "        t_data,\n",
    "        data[\"Vel_3_2D\"].reshape(-1, 1),\n",
    "        lambda t, u, X: differentiate_output(t, u, 1, 1),\n",
    "    ),\n",
    "    dde.icbc.PointSetOperatorBC(\n",
    "        t_data,\n",
    "        data[\"Vel_4_2D\"].reshape(-1, 1),\n",
    "        lambda t, u, X: differentiate_output(t, u, 3, 1),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Position BC\n",
    "xi = [\n",
    "    dde.icbc.PointSetBC(t_data, data[\"Disp_3_2D\"].reshape(-1, 1), component=1),\n",
    "    dde.icbc.PointSetBC(t_data, data[\"Disp_4_2D\"].reshape(-1, 1), component=3),\n",
    "]\n",
    "\n",
    "pde = dde.data.PDE(\n",
    "    geom,\n",
    "    ode_sys,\n",
    "    vi + xi,\n",
    "    num_domain=1000,\n",
    "    num_boundary=2,\n",
    "    anchors=data[\"t\"].reshape(-1, 1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure directories exist and are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folders:\n",
      "Checking model_files/checkpoints...\n",
      "Checking plots/training...\n"
     ]
    }
   ],
   "source": [
    "necessary_directories = [[\"model_files\", \"checkpoints\"], [\"plots\", \"training\"]]\n",
    "folders_created = []\n",
    "for dir in necessary_directories:\n",
    "    if not os.path.isdir(dir[0]):\n",
    "        os.mkdir(dir[0])\n",
    "        folders_created.append(dir[0])\n",
    "    if not os.path.isdir(f\"{dir[0]}/{dir[1]}\"):\n",
    "        os.mkdir(f\"{dir[0]}/{dir[1]}\")\n",
    "        folders_created.append(f\"{dir[0]}/{dir[1]}\")\n",
    "print(\"Created folders:\" + \"\\n> \".join([\"\", *folders_created]))\n",
    "\n",
    "## Ensure output files (model_files/checkpoints, plots/training) are empty.\n",
    "for path in [\"/\".join(entry) for entry in necessary_directories]:\n",
    "    print(f\"Checking {path}...\")\n",
    "    files = os.listdir(path)\n",
    "    if not files:\n",
    "        continue\n",
    "    print(f\"{path} not empty. Deleting contents...\")\n",
    "    for file in files:\n",
    "        filepath = os.path.join(path, file)\n",
    "        try:\n",
    "            os.unlink(filepath)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to delete %s. Reason: %s\" % (filepath, e))\n",
    "\n",
    "if not os.path.exists(\"out_files\"):\n",
    "    os.mkdir(\"out_files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "variable = dde.callbacks.VariableValue(var_list=[E], period=checkpoint_interval, filename=\"out_files/variables.dat\")\n",
    "\n",
    "plotter_callback = PlotterCallback(\n",
    "    period=checkpoint_interval, \n",
    "    filepath=\"plots/training\", \n",
    "    data=data,\n",
    "    E=E, \n",
    "    t_max=T_MAX, \n",
    "    u_max=U_MAX, \n",
    "    plot_residual=False\n",
    ")\n",
    "\n",
    "resampler = dde.callbacks.PDEPointResampler(period=10_000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Compile, and Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MsFNN(\n",
    "    layer_sizes=[1] + 10*[100] + [4],\n",
    "    activation=\"tanh\",\n",
    "    kernel_initializer=\"Glorot uniform\",\n",
    "    sigmas=[1, 10, 20, 50]\n",
    ")\n",
    "\n",
    "model = dde.Model(pde, net)\n",
    "model.compile(optimizer=\"adam\", lr=1e-4, external_trainable_variables=E)\n",
    "losshistory, train_state = model.train(\n",
    "    iterations=100_000, callbacks=[variable, plotter_callback, resampler]\n",
    ")\n",
    "\n",
    "\n",
    "dde.saveplot(\n",
    "    losshistory,\n",
    "    train_state,\n",
    "    loss_fname=\"out_files/loss.dat\",\n",
    "    train_fname=\"out_files/train.dat\",\n",
    "    test_fname=\"out_files/test.dat\",\n",
    ")\n",
    "model.save(\"model_files/msfnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

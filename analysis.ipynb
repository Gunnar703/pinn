{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four Degree-of-Freedom System\n",
    "\n",
    "### Known\n",
    "- $ \\left[ M \\right] $ \n",
    "- $ a_0,\\ a_1 $\n",
    "- $ \\left[ C \\right] = a_0 \\left[ M \\right] + a_1 \\left[ K \\right] $\n",
    "- $ K_{ij} \\geq 0\\ \\forall (i, j) \\in \\mathbb{N} \\times \\mathbb{N} $\n",
    "- $ \\left[ K \\right] $ is sparse\n",
    "- $ \\left[ K \\right] = \\sum_{i=1}^4 \\left[ \\mathbb{K}_\\text{basis} \\right]_i \\cdot E_i $\n",
    "\n",
    "### Unknown\n",
    "- $ \\mathbb{E} = \\bigcup_{i=1}^4 E_i $\n",
    "- $ \\alpha_\\pi $\n",
    "\n",
    "### Constraints\n",
    "- $ \\mathcal{J}_\\mathcal{D} = \\frac{1}{2} \\sum_{i=1,2} \\left( \\hat{u}_i - u_i \\right)^2 $ (data loss)\n",
    "- $ E_{ij} \\geq 0\\ \\forall (i, j) \\in \\mathbb{N} \\times \\mathbb{N} $ (hard constraint)\n",
    "- $ \\mathcal{J}_\\pi = \\alpha_\\pi \\mathcal{L}_2\\left( \\left[ M \\right]\\left[ \\ddot{u} \\right] + \\left[ C \\right]\\left[ \\dot{u} \\right] + \\left[ K \\right]\\left[ u \\right] - \\left[ f(t) \\right] \\right) $\n",
    "- $ \\mathcal{J}_\\mathcal{S} = \\mathcal{L}_1\\left( \\left[ K \\right] \\right) $ (sparsity enforcement, not used here because the 4DOF K-matrix is not actually sparse)\n",
    "\n",
    "### Definitions\n",
    "$\\mathcal{L}_1$: Taxicab norm\\\n",
    "$\\mathcal{L}_2$: Euclidiean norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "## Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.integrate import odeint\n",
    "print(\"Done.\")\n",
    "\n",
    "## Set hyperparameters\n",
    "np.random.seed(123)\n",
    "N_DEGREES_OF_FREEDOM = 4\n",
    "N_COLLOC_POINTS = 50\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining training data...\n",
      "True E: [0.69646919 0.28613933 0.22685145 0.55131477]\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining training data...\")\n",
    "## Define Known Values\n",
    "omega1 = 2 * np.pi * 1.5 #1.5 hz first mode\n",
    "omega2 = 2 * np.pi * 14\n",
    "damp1 = 0.01\n",
    "damp2 = 0.02\n",
    "a0 = ( 2 * damp1 * omega1 * (omega2**2) - 2 * damp2 * omega2 * (omega1**2) ) / ( (omega2**2) -(omega1**2) )\n",
    "a1 = ( 2 * damp2 * omega2 - 2 * damp1 * omega1 ) / ( (omega2**2) -(omega1**2) )\n",
    "\n",
    "m = np.diag( np.ones(N_DEGREES_OF_FREEDOM) )\n",
    "e = np.random.rand(N_DEGREES_OF_FREEDOM)\n",
    "print(f\"True E: {e}\")\n",
    "\n",
    "first_diag = int( np.floor(N_DEGREES_OF_FREEDOM*3/4) )\n",
    "k_basis = np.transpose(np.array([\n",
    "    np.diag(np.random.rand(N_DEGREES_OF_FREEDOM))\n",
    "    +\n",
    "    np.diag(np.random.rand(first_diag), k=N_DEGREES_OF_FREEDOM-first_diag) * 0.6\n",
    "    +\n",
    "    np.diag(np.random.rand(first_diag), k=-(N_DEGREES_OF_FREEDOM-first_diag)) * 0.6\n",
    "    for _ in range(N_DEGREES_OF_FREEDOM)\n",
    "]), axes=(1, 0, 2))\n",
    "\n",
    "k = np.dot(e, k_basis)\n",
    "c = a0 * m + a1 * k\n",
    "\n",
    "force_index = 1\n",
    "def np_force(t):\n",
    "    force_mask = np.zeros( N_DEGREES_OF_FREEDOM ).reshape(-1, 1)\n",
    "    force_mask[force_index] = 1\n",
    "    return np.exp(-(t-np.pi)**2) * np.sin(2*np.pi*t) * force_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solve ODE\n",
    "def ode(u, t):\n",
    "    y    = u[ 0 : N_DEGREES_OF_FREEDOM ].reshape(-1, 1)\n",
    "    y_t  = u[ N_DEGREES_OF_FREEDOM : ].reshape(-1, 1)\n",
    "            \n",
    "    y_tt = np.linalg.inv(m) @ (\n",
    "        np_force(t)\n",
    "        -\n",
    "        c @ y_t\n",
    "        -\n",
    "        k @ y\n",
    "    )\n",
    "    return np.array( list( y_t.squeeze() ) + list(y_tt.squeeze()) )\n",
    "\n",
    "u0 = np.zeros( N_DEGREES_OF_FREEDOM * 2 )\n",
    "t  = np.linspace( 0, 4 * np.pi, 150 )\n",
    "\n",
    "sol = odeint(ode, u0, t)\n",
    "u   = sol[:, :N_DEGREES_OF_FREEDOM]\n",
    "u_t = sol[:, N_DEGREES_OF_FREEDOM:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Plotting...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "## Screen out training data\n",
    "time_indices   = np.arange( 0, len(t), len(t) // N_COLLOC_POINTS )\n",
    "sensor_indices = [1, 3]\n",
    "\n",
    "tdata = t[time_indices]\n",
    "udata = u[time_indices]  # this data includes all dimensions, not just the sensor dimensions\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Plotting...\")\n",
    "## Plot\n",
    "fig, ax = plt.subplots(N_DEGREES_OF_FREEDOM, 1, sharex=True)\n",
    "plt.suptitle(\"Solution and Data\")\n",
    "for dim in range(N_DEGREES_OF_FREEDOM):\n",
    "    ax[dim].plot(t, u[:, dim], label=\"Solution\")\n",
    "    if dim in sensor_indices:\n",
    "        ax[dim].plot(tdata, udata[:, dim], label=\"Data\", linestyle=\"None\", marker=\".\")\n",
    "    ax[dim].set_xlabel(r\"$t$\")\n",
    "    ax[dim].set_ylabel(r\"$u_{}(t)$\".format(dim + 1))\n",
    "    if dim == 0:\n",
    "        ax[dim].legend(loc=\"upper right\", ncol=2)\n",
    "plt.savefig(\"plots/training_data.png\")\n",
    "plt.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up DeepXDE model...\n",
      "Compiling model...\n",
      "'compile' took 0.000260 s\n",
      "\n",
      "Done.\n",
      "Training model...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m checkpoint\u001b[39m.\u001b[39mon_epoch_begin \u001b[39m=\u001b[39m plot\n\u001b[0;32m     98\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m losshistory, train_state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(iterations\u001b[39m=\u001b[39;49m\u001b[39m2_000_000\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[variable, checkpoint])\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\utils\\internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     21\u001b[0m     ts \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 22\u001b[0m     result \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     23\u001b[0m     te \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m took \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m s\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, te \u001b[39m-\u001b[39m ts))\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\model.py:610\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mset_data_train(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtrain_next_batch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size))\n\u001b[0;32m    609\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mset_data_test(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtest())\n\u001b[1;32m--> 610\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test()\n\u001b[0;32m    611\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_train_begin()\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m optimizers\u001b[39m.\u001b[39mis_external_optimizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_name):\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\model.py:797\u001b[0m, in \u001b[0;36mModel._test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_test\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    794\u001b[0m     (\n\u001b[0;32m    795\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39my_pred_train,\n\u001b[0;32m    796\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mloss_train,\n\u001b[1;32m--> 797\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outputs_losses(\n\u001b[0;32m    798\u001b[0m         \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    799\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_state\u001b[39m.\u001b[39;49mX_train,\n\u001b[0;32m    800\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_state\u001b[39m.\u001b[39;49my_train,\n\u001b[0;32m    801\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_state\u001b[39m.\u001b[39;49mtrain_aux_vars,\n\u001b[0;32m    802\u001b[0m     )\n\u001b[0;32m    803\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39my_pred_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mloss_test \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_losses(\n\u001b[0;32m    804\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    805\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mX_test,\n\u001b[0;32m    806\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39my_test,\n\u001b[0;32m    807\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39mtest_aux_vars,\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state\u001b[39m.\u001b[39my_test, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\model.py:522\u001b[0m, in \u001b[0;36mModel._outputs_losses\u001b[1;34m(self, training, inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39melif\u001b[39;00m backend_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    520\u001b[0m     \u001b[39m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[0;32m    521\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mrequires_grad_(requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 522\u001b[0m     outs \u001b[39m=\u001b[39m outputs_losses(inputs, targets)\n\u001b[0;32m    523\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[0;32m    524\u001b[0m \u001b[39melif\u001b[39;00m backend_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjax\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    525\u001b[0m     \u001b[39m# TODO: auxiliary_vars\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\model.py:303\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.outputs_losses_train\u001b[1;34m(inputs, targets)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moutputs_losses_train\u001b[39m(inputs, targets):\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs_losses(\u001b[39mTrue\u001b[39;49;00m, inputs, targets, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mlosses_train)\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\model.py:287\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.outputs_losses\u001b[1;34m(training, inputs, targets, losses_fn)\u001b[0m\n\u001b[0;32m    285\u001b[0m     inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(inputs)\n\u001b[0;32m    286\u001b[0m     inputs\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m--> 287\u001b[0m outputs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(inputs)\n\u001b[0;32m    288\u001b[0m \u001b[39m# Data losses\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\deepxde\\nn\\pytorch\\fnn.py:43\u001b[0m, in \u001b[0;36mFNN.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_transform(x)\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m j, linear \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinears[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[0;32m     40\u001b[0m     x \u001b[39m=\u001b[39m (\n\u001b[0;32m     41\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation[j](linear(x))\n\u001b[0;32m     42\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation, \u001b[39mlist\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(linear(x))\n\u001b[0;32m     44\u001b[0m     )\n\u001b[0;32m     45\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinears[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](x)\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aglor\\anaconda3\\envs\\pinn\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Set up DeepXDE model\n",
    "print(\"Setting up DeepXDE model...\")\n",
    "# Define domain\n",
    "geometry = dde.geometry.TimeDomain( t[0], t[-1] )\n",
    "\n",
    "# Define forcing function\n",
    "def pt_force(t):\n",
    "    return torch.cat(\n",
    "        [\n",
    "            (torch.exp(-(t-np.pi)**2) * torch.sin(2*np.pi*t)).view(1, -1) if dim == force_index else (t * 0).reshape(1, -1)\n",
    "            for dim in range(N_DEGREES_OF_FREEDOM)\n",
    "        ],\n",
    "        axis = 0\n",
    "    )\n",
    "\n",
    "# Define parameters\n",
    "E        = dde.Variable( np.ones_like( e ), dtype=torch.float32 )\n",
    "alpha_pi = dde.Variable(1.0)\n",
    "\n",
    "# Define other tensors\n",
    "M = torch.Tensor(m)\n",
    "K_basis = torch.Tensor(k_basis)\n",
    "\n",
    "# Define the ODE residual\n",
    "def system (t, u):\n",
    "    y    = u\n",
    "    y_t  = torch.zeros_like( y ).to(device)\n",
    "    y_tt = torch.zeros_like( y ).to(device)\n",
    "    \n",
    "    for dim in range( N_DEGREES_OF_FREEDOM ):\n",
    "        y_t [:, dim] = dde.grad.jacobian( u, t, i=dim, j=0 ).squeeze()\n",
    "        y_tt[:, dim] = dde.grad.hessian ( u, t, component=dim ).squeeze()\n",
    "    \n",
    "    E = torch.abs(E)\n",
    "    K = torch.matmul( E, K_basis )\n",
    "    C = a0 * M + a1 * K\n",
    "            \n",
    "    residual = (\n",
    "        torch.mm( M, y_tt.permute((1, 0)) )\n",
    "        +\n",
    "        torch.mm( torch.abs(C), y_t.permute((1, 0)) )\n",
    "        +\n",
    "        torch.mm( torch.abs(K), y.permute((1, 0)) )\n",
    "        -\n",
    "        pt_force(t)\n",
    "    ).permute((1, 0))\n",
    "    return alpha_pi * residual\n",
    "\n",
    "bcs = [\n",
    "    dde.icbc.boundary_conditions.PointSetBC( tdata.reshape(-1, 1), udata[:, dim].reshape(-1, 1), component=dim )\n",
    "    for dim in sensor_indices\n",
    "]\n",
    "\n",
    "data = dde.data.PDE(\n",
    "    geometry     = geometry,\n",
    "    pde          = system,\n",
    "    bcs          = bcs,\n",
    "    num_domain   = 10000,\n",
    "    num_boundary = 2,\n",
    "    num_test     = 5\n",
    ")\n",
    "\n",
    "net = dde.nn.FNN(\n",
    "    layer_sizes        = [1] + 20*[32] + [N_DEGREES_OF_FREEDOM],\n",
    "    activation         = \"tanh\",\n",
    "    kernel_initializer = \"Glorot uniform\"\n",
    ")\n",
    "\n",
    "model = dde.Model(data, net)\n",
    "model.compile(\"adam\", lr=1e-4, external_trainable_variables=[E, alpha_pi])\n",
    "\n",
    "variable = dde.callbacks.VariableValue(\n",
    "  list(E) + [alpha_pi], period=1000, filename=\"variables.dat\"\n",
    ")\n",
    "\n",
    "checkpoint = dde.callbacks.ModelCheckpoint(\"model_files/checkpoints/model\", period=10_000)\n",
    "\n",
    "epoch = 0\n",
    "def plot():\n",
    "    global epoch\n",
    "    epoch += 1\n",
    "    if checkpoint.epochs_since_last_save + 1 < checkpoint.period: return\n",
    "    upred = model.predict(t.reshape(-1, 1))\n",
    "    _, ax = plt.subplots(N_DEGREES_OF_FREEDOM, 1, sharex=True)\n",
    "    plt.suptitle(f\"Epoch {epoch}\")\n",
    "    for dim in range(N_DEGREES_OF_FREEDOM):\n",
    "        ax[dim].plot(t, u[:, dim], label=\"Solution\", color='blue')\n",
    "        if dim in sensor_indices:\n",
    "            ax[dim].plot(tdata, udata[:, dim], label=\"Data\", linestyle=\"None\", marker=\".\", color='orange')\n",
    "        ax[dim].plot(t, upred[:, dim], label=\"Prediction\", color='green')\n",
    "        ax[dim].set_xlabel(r\"$t$\")\n",
    "        ax[dim].set_ylabel(r\"$u_{}(t)$\".format(dim + 1))\n",
    "        if dim == 0:\n",
    "            ax[dim].legend(loc=\"upper right\", ncol=2)\n",
    "    plt.savefig(f\"plots/training/epoch_{epoch}_prediction.png\")\n",
    "    plt.close()\n",
    "\n",
    "checkpoint.on_epoch_begin = plot\n",
    "print(\"Done.\")\n",
    "losshistory, train_state = model.train(iterations=2_000_000, callbacks=[variable, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model...\")\n",
    "model.save(\"model_files/model\")\n",
    "dde.utils.saveplot(losshistory, train_state, issave=True, isplot=True)\n",
    "print(\"Done.\")\n",
    "\n",
    "#### Print final E vector #####\n",
    "print(\"Final learned E vector\\n\", \"----------\")\n",
    "print(\"E = \\n\", E.detach())\n",
    "\n",
    "print(\"True E vector\\n\", \"----------\")\n",
    "print(\"EK = \\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
